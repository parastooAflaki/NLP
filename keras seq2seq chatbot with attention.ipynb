{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random \n",
    "import os\n",
    "import numpy as np\n",
    "data_path = \"/home/parastoo/Downloads/human_text.txt\"\n",
    "data_path2 = \"/home/parastoo/Downloads/robot_text.txt\"\n",
    "path = \"/home/parastoo/Downloads/glove.6B(1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>', U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from numpy import array\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lines as a list of each line\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "  lines = f.read().split('\\n')\n",
    "with open(data_path2, 'r', encoding='utf-8') as f:\n",
    "  lines2 = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"i'm\", \"i am\", txt)\n",
    "    txt = re.sub(r\"he's\", \"he is\", txt)\n",
    "    txt = re.sub(r\"she's\", \"she is\", txt)\n",
    "    txt = re.sub(r\"that's\", \"that is\", txt)\n",
    "    txt = re.sub(r\"what's\", \"what is\", txt)\n",
    "    txt = re.sub(r\"where's\", \"where is\", txt)\n",
    "    txt = re.sub(r\"\\'ll\", \" will\", txt)\n",
    "    txt = re.sub(r\"\\'ve\", \" have\", txt)\n",
    "    txt = re.sub(r\"\\'re\", \" are\", txt)\n",
    "    txt = re.sub(r\"\\'d\", \" would\", txt)\n",
    "    txt = re.sub(r\"won't\", \"will not\", txt)\n",
    "    txt = re.sub(r\"can't\", \"can not\", txt)\n",
    "    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [clean_text(line) for line in lines]\n",
    "lines2 = [clean_text(line) for line in lines2]\n",
    "\n",
    "# grouping lines by response pair\n",
    "pairs = list(zip(lines,lines2))\n",
    "#random.shuffle(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2363\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(len(pd.DataFrame(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start</td>\n",
       "      <td>hi there how are you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oh thanks   i am fine this is an evening in my...</td>\n",
       "      <td>here is afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how do you feel today   tell me something abou...</td>\n",
       "      <td>my name is rdany but you can call me dany the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how many virtual friends have you got</td>\n",
       "      <td>i have many   but not enough to fully understa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is that forbidden for you to tell the exact nu...</td>\n",
       "      <td>i have talked with 143 users counting 7294 lin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0                                              start   \n",
       "1  oh thanks   i am fine this is an evening in my...   \n",
       "2  how do you feel today   tell me something abou...   \n",
       "3            how many virtual friends have you got     \n",
       "4  is that forbidden for you to tell the exact nu...   \n",
       "\n",
       "                                                   1  \n",
       "0                          hi there how are you       \n",
       "1                                here is afternoon    \n",
       "2  my name is rdany but you can call me dany the ...  \n",
       "3  i have many   but not enough to fully understa...  \n",
       "4  i have talked with 143 users counting 7294 lin...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pairs).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max input length:  1266\n",
      "max target length:  935\n"
     ]
    }
   ],
   "source": [
    "max_input_length = max([len(line) for line in lines])\n",
    "max_target_length = max([len(line) for line in lines2])\n",
    "print(\"max input length: \", max_input_length)\n",
    "print(\"max target length: \", max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lines2)):\n",
    "    lines2[i] = ' '.join(lines2[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "\n",
    "for line in lines:\n",
    "    for word in line.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "for line in lines2:\n",
    "    for word in line.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 5\n",
    "\n",
    "vocab = {}\n",
    "word_num = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= thresh:\n",
    "        vocab[word] = word_num\n",
    "        word_num += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS> hi there how are you <EOS>',\n",
       " '<SOS> here is afternoon <EOS>',\n",
       " '<SOS> my name is rdany but you can call me dany the r means robot i hope we can be virtual friends <EOS>',\n",
       " '<SOS> i have many but not enough to fully understand humans beings <EOS>',\n",
       " '<SOS> i have talked with 143 users counting 7294 lines of text <EOS>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(lines2)):\n",
    "    lines2[i] = '<SOS> ' + lines2[i] + ' <EOS>'\n",
    "\n",
    "\n",
    "\n",
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "x = len(vocab)\n",
    "for token in tokens:\n",
    "    vocab[token] = x\n",
    "    x += 1\n",
    "    \n",
    "    \n",
    "\n",
    "vocab['cameron'] = vocab['<PAD>']\n",
    "vocab['<PAD>'] = 0\n",
    "lines2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>oh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1\n",
       "0  0   <PAD>\n",
       "1  1      oh\n",
       "2  2  thanks\n",
       "3  3       i\n",
       "4  4      am"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_vocab = {w:v for v, w in vocab.items()}\n",
    "pd.DataFrame(inv_vocab.items()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>915</td>\n",
       "      <td>94</td>\n",
       "      <td>311.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>915</td>\n",
       "      <td>230</td>\n",
       "      <td>7.0</td>\n",
       "      <td>914.0</td>\n",
       "      <td>913.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>915</td>\n",
       "      <td>11</td>\n",
       "      <td>87.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>915</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>873.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>915</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>914.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>914.0</td>\n",
       "      <td>914.0</td>\n",
       "      <td>914.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1      2      3      4      5      6      7      8      9    ...  136  \\\n",
       "0  915   94  311.0   13.0   40.0   15.0  913.0    NaN    NaN    NaN  ...  NaN   \n",
       "1  915  230    7.0  914.0  913.0    NaN    NaN    NaN    NaN    NaN  ...  NaN   \n",
       "2  915   11   87.0    7.0  134.0   81.0   15.0   53.0  302.0   19.0  ...  NaN   \n",
       "3  915    3   26.0   23.0   81.0   54.0  561.0   30.0  873.0  371.0  ...  NaN   \n",
       "4  915    3   26.0  122.0   47.0  914.0  700.0  914.0  914.0  914.0  ...  NaN   \n",
       "\n",
       "   137  138  139  140  141  142  143  144  145  \n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 146 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inp = []\n",
    "for line in lines:\n",
    "    lst = []\n",
    "    for word in line.split():\n",
    "        if word not in vocab:\n",
    "            lst.append(vocab['<OUT>'])\n",
    "        else:\n",
    "            lst.append(vocab[word])\n",
    "        \n",
    "    encoder_inp.append(lst)\n",
    "\n",
    "decoder_inp = []\n",
    "for line in lines2:\n",
    "    lst = []\n",
    "    for word in line.split():\n",
    "        if word not in vocab:\n",
    "            lst.append(vocab['<OUT>'])\n",
    "        else:\n",
    "            lst.append(vocab[word])        \n",
    "    decoder_inp.append(lst)\n",
    "\n",
    "pd.DataFrame(decoder_inp).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>915</td>\n",
       "      <td>94</td>\n",
       "      <td>311</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>915</td>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>914</td>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>915</td>\n",
       "      <td>11</td>\n",
       "      <td>87</td>\n",
       "      <td>7</td>\n",
       "      <td>134</td>\n",
       "      <td>81</td>\n",
       "      <td>15</td>\n",
       "      <td>53</td>\n",
       "      <td>302</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>915</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>81</td>\n",
       "      <td>54</td>\n",
       "      <td>561</td>\n",
       "      <td>30</td>\n",
       "      <td>873</td>\n",
       "      <td>371</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>915</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>122</td>\n",
       "      <td>47</td>\n",
       "      <td>914</td>\n",
       "      <td>700</td>\n",
       "      <td>914</td>\n",
       "      <td>914</td>\n",
       "      <td>914</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9   ...  40  41  42  43  44  \\\n",
       "0  915   94  311   13   40   15  913    0    0    0  ...   0   0   0   0   0   \n",
       "1  915  230    7  914  913    0    0    0    0    0  ...   0   0   0   0   0   \n",
       "2  915   11   87    7  134   81   15   53  302   19  ...   0   0   0   0   0   \n",
       "3  915    3   26   23   81   54  561   30  873  371  ...   0   0   0   0   0   \n",
       "4  915    3   26  122   47  914  700  914  914  914  ...   0   0   0   0   0   \n",
       "\n",
       "   45  46  47  48  49  \n",
       "0   0   0   0   0   0  \n",
       "1   0   0   0   0   0  \n",
       "2   0   0   0   0   0  \n",
       "3   0   0   0   0   0  \n",
       "4   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoder_inp = pad_sequences(encoder_inp, 50, padding='post', truncating='post')\n",
    "decoder_inp = pad_sequences(decoder_inp, 50, padding='post', truncating='post')\n",
    "pd.DataFrame(decoder_inp).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94</td>\n",
       "      <td>311</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230</td>\n",
       "      <td>7</td>\n",
       "      <td>914</td>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>87</td>\n",
       "      <td>7</td>\n",
       "      <td>134</td>\n",
       "      <td>81</td>\n",
       "      <td>15</td>\n",
       "      <td>53</td>\n",
       "      <td>302</td>\n",
       "      <td>19</td>\n",
       "      <td>415</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>81</td>\n",
       "      <td>54</td>\n",
       "      <td>561</td>\n",
       "      <td>30</td>\n",
       "      <td>873</td>\n",
       "      <td>371</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>122</td>\n",
       "      <td>47</td>\n",
       "      <td>914</td>\n",
       "      <td>700</td>\n",
       "      <td>914</td>\n",
       "      <td>914</td>\n",
       "      <td>914</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>94</td>\n",
       "      <td>230</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>97</td>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>75</td>\n",
       "      <td>755</td>\n",
       "      <td>914</td>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>94</td>\n",
       "      <td>311</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2362</th>\n",
       "      <td>94</td>\n",
       "      <td>913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2363 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9   ...  40  41  42  43  \\\n",
       "0      94  311   13   40   15  913    0    0    0    0  ...   0   0   0   0   \n",
       "1     230    7  914  913    0    0    0    0    0    0  ...   0   0   0   0   \n",
       "2      11   87    7  134   81   15   53  302   19  415  ...   0   0   0   0   \n",
       "3       3   26   23   81   54  561   30  873  371   80  ...   0   0   0   0   \n",
       "4       3   26  122   47  914  700  914  914  914   82  ...   0   0   0   0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ..  ..  ..  ..   \n",
       "2358   94  230   13   40   15  913    0    0    0    0  ...   0   0   0   0   \n",
       "2359   98    3    4    5   97  913    0    0    0    0  ...   0   0   0   0   \n",
       "2360   75  755  914  913    0    0    0    0    0    0  ...   0   0   0   0   \n",
       "2361   94  311   13   40   15  913    0    0    0    0  ...   0   0   0   0   \n",
       "2362   94  913    0    0    0    0    0    0    0    0  ...   0   0   0   0   \n",
       "\n",
       "      44  45  46  47  48  49  \n",
       "0      0   0   0   0   0   0  \n",
       "1      0   0   0   0   0   0  \n",
       "2      0   0   0   0   0   0  \n",
       "3      0   0   0   0   0   0  \n",
       "4      0   0   0   0   0   0  \n",
       "...   ..  ..  ..  ..  ..  ..  \n",
       "2358   0   0   0   0   0   0  \n",
       "2359   0   0   0   0   0   0  \n",
       "2360   0   0   0   0   0   0  \n",
       "2361   0   0   0   0   0   0  \n",
       "2362   0   0   0   0   0   0  \n",
       "\n",
       "[2363 rows x 50 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_final_output = []\n",
    "for i in decoder_inp:\n",
    "    decoder_final_output.append(i[1:]) \n",
    "decoder_final_output = pad_sequences(decoder_final_output, 50, padding='post', truncating='post')\n",
    "pd.DataFrame(decoder_final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "decoder_final_output = to_categorical(decoder_final_output, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Loded!\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('/home/parastoo/Downloads/glove.6B(1)/glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "print(\"Glove Loded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimention = 50\n",
    "def embedding_matrix_creater(embedding_dimention, word_index):\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dimention))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "embedding_matrix = embedding_matrix_creater(50, word_index=vocab)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(918, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate, Dropout, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "MAX_LEN = 50\n",
    "\n",
    "embed = Embedding(VOCAB_SIZE+1, \n",
    "                  50, \n",
    "                  \n",
    "                  input_length=50,\n",
    "                  trainable=True)\n",
    "\n",
    "embed.build((None,))\n",
    "embed.set_weights([embedding_matrix])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 50)       45900       input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   [(None, 50, 800), (N 1443200     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 800)          0           bidirectional[0][1]              \n",
      "                                                                 bidirectional[0][3]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 800)          0           bidirectional[0][2]              \n",
      "                                                                 bidirectional[0][4]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 800), (N 2723200     embedding[1][0]                  \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, 50, 800), (N 1280800     bidirectional[0][0]              \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 50, 1600)     0           lstm_1[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50, 917)      1468117     concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,961,217\n",
      "Trainable params: 6,961,217\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc_inp = Input(shape=(50, ))\n",
    "#embed = Embedding(VOCAB_SIZE+1, 50, mask_zero=True, input_length=13)(enc_inp)\n",
    "enc_embed = embed(enc_inp)\n",
    "enc_lstm = Bidirectional(LSTM(400, return_state=True, dropout=0.05, return_sequences = True))\n",
    "\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = enc_lstm(enc_embed)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "enc_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "dec_inp = Input(shape=(50, ))\n",
    "dec_embed = embed(dec_inp)\n",
    "dec_lstm = LSTM(400*2, return_state=True, return_sequences=True, dropout=0.05)\n",
    "output, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n",
    "\n",
    "# attention\n",
    "attn_layer = AttentionLayer()\n",
    "attn_op, attn_state = attn_layer([encoder_outputs, output])\n",
    "decoder_concat_input = Concatenate(axis=-1)([output, attn_op])\n",
    "\n",
    "\n",
    "dec_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
    "final_output = dec_dense(decoder_concat_input)\n",
    "\n",
    "model = Model([enc_inp, dec_inp], final_output)\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "84/84 [==============================] - 81s 962ms/step - loss: 1.1325 - acc: 0.8149 - val_loss: 0.8535 - val_acc: 0.8534\n",
      "Epoch 2/30\n",
      "84/84 [==============================] - 81s 966ms/step - loss: 0.8467 - acc: 0.8472 - val_loss: 0.7212 - val_acc: 0.8724\n",
      "Epoch 3/30\n",
      "84/84 [==============================] - 81s 967ms/step - loss: 0.7745 - acc: 0.8567 - val_loss: 0.6885 - val_acc: 0.8773\n",
      "Epoch 4/30\n",
      "84/84 [==============================] - 82s 973ms/step - loss: 0.7228 - acc: 0.8621 - val_loss: 0.6526 - val_acc: 0.8817\n",
      "Epoch 5/30\n",
      "84/84 [==============================] - 81s 964ms/step - loss: 0.6789 - acc: 0.8658 - val_loss: 0.6419 - val_acc: 0.8824\n",
      "Epoch 6/30\n",
      "84/84 [==============================] - 81s 965ms/step - loss: 0.6374 - acc: 0.8689 - val_loss: 0.6378 - val_acc: 0.8814\n",
      "Epoch 7/30\n",
      "84/84 [==============================] - 81s 967ms/step - loss: 0.5974 - acc: 0.8726 - val_loss: 0.6311 - val_acc: 0.8845\n",
      "Epoch 8/30\n",
      "84/84 [==============================] - 82s 978ms/step - loss: 0.5528 - acc: 0.8769 - val_loss: 0.6433 - val_acc: 0.8862\n",
      "Epoch 9/30\n",
      "84/84 [==============================] - 82s 982ms/step - loss: 0.5079 - acc: 0.8816 - val_loss: 0.6483 - val_acc: 0.8857\n",
      "Epoch 10/30\n",
      "84/84 [==============================] - 82s 981ms/step - loss: 0.4591 - acc: 0.8882 - val_loss: 0.6653 - val_acc: 0.8842\n",
      "Epoch 11/30\n",
      "84/84 [==============================] - 82s 980ms/step - loss: 0.4069 - acc: 0.8976 - val_loss: 0.6841 - val_acc: 0.8806\n",
      "Epoch 12/30\n",
      "84/84 [==============================] - 82s 981ms/step - loss: 0.3545 - acc: 0.9085 - val_loss: 0.6968 - val_acc: 0.8840\n",
      "Epoch 13/30\n",
      "84/84 [==============================] - 82s 981ms/step - loss: 0.3034 - acc: 0.9205 - val_loss: 0.7138 - val_acc: 0.8826\n",
      "Epoch 14/30\n",
      "84/84 [==============================] - 82s 980ms/step - loss: 0.2558 - acc: 0.9331 - val_loss: 0.7309 - val_acc: 0.8827\n",
      "Epoch 15/30\n",
      "84/84 [==============================] - 82s 980ms/step - loss: 0.2077 - acc: 0.9471 - val_loss: 0.7466 - val_acc: 0.8814\n",
      "Epoch 16/30\n",
      "84/84 [==============================] - 82s 980ms/step - loss: 0.1688 - acc: 0.9576 - val_loss: 0.7742 - val_acc: 0.8820\n",
      "Epoch 17/30\n",
      "84/84 [==============================] - 82s 980ms/step - loss: 0.1357 - acc: 0.9677 - val_loss: 0.7974 - val_acc: 0.8802\n",
      "Epoch 18/30\n",
      "84/84 [==============================] - 82s 981ms/step - loss: 0.1057 - acc: 0.9763 - val_loss: 0.8154 - val_acc: 0.8825\n",
      "Epoch 19/30\n",
      "84/84 [==============================] - 82s 980ms/step - loss: 0.0856 - acc: 0.9813 - val_loss: 0.8345 - val_acc: 0.8819\n",
      "Epoch 20/30\n",
      "84/84 [==============================] - 82s 981ms/step - loss: 0.0674 - acc: 0.9855 - val_loss: 0.8572 - val_acc: 0.8817\n",
      "Epoch 21/30\n",
      "84/84 [==============================] - 82s 981ms/step - loss: 0.0542 - acc: 0.9887 - val_loss: 0.8786 - val_acc: 0.8825\n",
      "Epoch 22/30\n",
      "84/84 [==============================] - 82s 980ms/step - loss: 0.0458 - acc: 0.9902 - val_loss: 0.8790 - val_acc: 0.8815\n",
      "Epoch 23/30\n",
      "84/84 [==============================] - 82s 980ms/step - loss: 0.0381 - acc: 0.9922 - val_loss: 0.8961 - val_acc: 0.8816\n",
      "Epoch 24/30\n",
      "84/84 [==============================] - 82s 982ms/step - loss: 0.0328 - acc: 0.9930 - val_loss: 0.9082 - val_acc: 0.8798\n",
      "Epoch 25/30\n",
      "84/84 [==============================] - 82s 981ms/step - loss: 0.0288 - acc: 0.9938 - val_loss: 0.9246 - val_acc: 0.8820\n",
      "Epoch 26/30\n",
      "84/84 [==============================] - 82s 981ms/step - loss: 0.0265 - acc: 0.9943 - val_loss: 0.9280 - val_acc: 0.8810\n",
      "Epoch 27/30\n",
      "84/84 [==============================] - 82s 981ms/step - loss: 0.0247 - acc: 0.9944 - val_loss: 0.9466 - val_acc: 0.8828\n",
      "Epoch 28/30\n",
      "84/84 [==============================] - 83s 982ms/step - loss: 0.0242 - acc: 0.9945 - val_loss: 0.9422 - val_acc: 0.8801\n",
      "Epoch 29/30\n",
      "84/84 [==============================] - 82s 981ms/step - loss: 0.0233 - acc: 0.9948 - val_loss: 0.9404 - val_acc: 0.8802\n",
      "Epoch 30/30\n",
      "84/84 [==============================] - 82s 981ms/step - loss: 0.0229 - acc: 0.9948 - val_loss: 0.9665 - val_acc: 0.8812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd2080355e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.fit([encoder_inp, decoder_inp], decoder_final_output, epochs=30\n",
    "          , batch_size=24, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2363, 50, 917)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model = tf.keras.models.Model(enc_inp, [encoder_outputs, enc_states])\n",
    "\n",
    "\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=( 400 * 2,))\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=( 400 * 2,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "\n",
    "decoder_outputs, state_h, state_c = dec_lstm(dec_embed , initial_state=decoder_states_inputs)\n",
    "\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "#decoder_output = dec_dense(decoder_outputs)\n",
    "\n",
    "dec_model = tf.keras.models.Model([dec_inp, decoder_states_inputs],\n",
    "                                      [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "#       start chatting ver. 1.0          #\n",
      "##########################################\n",
      "you : hi\n",
      "chatbot attention :  hi how are you \n",
      "==============================================\n",
      "you : i am fine\n",
      "chatbot attention :  good do you speak english \n",
      "==============================================\n",
      "you : yes i speak english and persian\n",
      "chatbot attention :  nice quite <OUT> \n",
      "==============================================\n",
      "you : do you like coffee?\n",
      "chatbot attention :  sure what is your favorite group \n",
      "==============================================\n",
      "you : how old are you\n",
      "chatbot attention :  i am 22 <OUT> but <OUT> \n",
      "==============================================\n",
      "you : i am 21.\n",
      "chatbot attention :  nice humans really love food \n",
      "==============================================\n",
      "you : yes i love food too\n",
      "chatbot attention :  \n",
      "==============================================\n",
      "you : what is yout name\n",
      "chatbot attention :  special <OUT> <OUT> <OUT> to create something new \n",
      "==============================================\n",
      "you : what is your name\n",
      "chatbot attention :  my name is rdany but you can call me dany \n",
      "==============================================\n",
      "you : do you like playing football ?\n",
      "chatbot attention :  i am a robot <OUT> the world \n",
      "==============================================\n",
      "you : where are you from?\n",
      "chatbot attention :  i am form a lab on argentina nice place when you get used to \n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "print(\"##########################################\")\n",
    "print(\"#       start chatting ver. 1.0          #\")\n",
    "print(\"##########################################\")\n",
    "\n",
    "\n",
    "prepro1 = \"\"\n",
    "while prepro1 != 'q':\n",
    "    \n",
    "    prepro1 = input(\"you : \")\n",
    "    try:\n",
    "        prepro1 = clean_text(prepro1)\n",
    "        prepro = [prepro1]\n",
    "        \n",
    "        txt = []\n",
    "        for x in prepro:\n",
    "            lst = []\n",
    "            for y in x.split():\n",
    "                try:\n",
    "                    lst.append(vocab[y])\n",
    "                except:\n",
    "                    lst.append(vocab['<OUT>'])\n",
    "            txt.append(lst)\n",
    "        txt = pad_sequences(txt, 13, padding='post')\n",
    "\n",
    "\n",
    "        ###\n",
    "        enc_op, stat = enc_model.predict( txt )\n",
    "\n",
    "        empty_target_seq = np.zeros( ( 1 , 1) )\n",
    "        empty_target_seq[0, 0] = vocab['<SOS>']\n",
    "        stop_condition = False\n",
    "        decoded_translation = ''\n",
    "\n",
    "\n",
    "        while not stop_condition :\n",
    "\n",
    "            dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + stat )\n",
    "\n",
    "            ###\n",
    "            ###########################\n",
    "            attn_op, attn_state = attn_layer([enc_op, dec_outputs])\n",
    "            decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])\n",
    "            decoder_concat_input = dec_dense(decoder_concat_input)\n",
    "            ###########################\n",
    "\n",
    "            sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n",
    "\n",
    "            sampled_word = inv_vocab[sampled_word_index] + ' '\n",
    "\n",
    "            if sampled_word != '<EOS> ':\n",
    "                decoded_translation += sampled_word           \n",
    "\n",
    "\n",
    "            if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n",
    "                stop_condition = True\n",
    "\n",
    "            empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "            empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "            stat = [ h , c ] \n",
    "\n",
    "        print(\"chatbot attention : \", decoded_translation )\n",
    "        print(\"==============================================\")\n",
    "\n",
    "    except:\n",
    "        print(\"sorry didn't got you , please type again :( \")\n",
    "ou\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_docs = []\n",
    "# target_docs = []\n",
    "# input_tokens = set()\n",
    "# target_tokens = set()\n",
    "\n",
    "# for line in pairs:\n",
    "#   input_doc, target_doc = line[0], line[1]\n",
    "#   input_docs.append(input_doc)\n",
    "  \n",
    "#   target_doc = '<START> ' + target_doc + ' <END>'\n",
    "#   target_docs.append(target_doc)\n",
    "  \n",
    "#   for token in input_doc.split():\n",
    "#     if token not in input_tokens:\n",
    "#       input_tokens.add(token)\n",
    "\n",
    "#   for token in target_doc.split():\n",
    "#     if token not in target_tokens:\n",
    "#       target_tokens.add(token)\n",
    "# input_tokens = sorted(set(list(input_tokens)))\n",
    "# target_tokens = sorted(set(list(target_tokens)))\n",
    "\n",
    "# num_encoder_tokens = len(input_tokens)\n",
    "# num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "# print(\" num of input unique tokens \", num_encoder_tokens ,\"\\n num of target uniquie tokens\" ,num_decoder_tokens)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set(input_tokens + target_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_features_dict = dict(\n",
    "#     [(token, i) for i, token in enumerate(input_tokens)])\n",
    "# target_features_dict = dict(\n",
    "#     [(token, i) for i, token in enumerate(target_tokens)])\n",
    "# reverse_input_features_dict = dict(\n",
    "#     (i, token) for token, i in input_features_dict.items())\n",
    "# reverse_target_features_dict = dict(\n",
    "#     (i, token) for token, i in target_features_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(input_features_dict.items(),columns={\"word\",\"index\"})[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# VOCAB_SIZE = 14999\n",
    "# tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "# tokenizer.fit_on_texts(input_docs + target_docs)\n",
    "# dictionary = tokenizer.word_index\n",
    "\n",
    "# input_seq = tokenizer.texts_to_sequences(input_docs)\n",
    "# target_seq = tokenizer.texts_to_sequences(target_docs)\n",
    "\n",
    "# #** fit on text starts the indexes from one not zero. which is good for padding\n",
    "# pd.DataFrame(dictionary.items()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2idx = {}\n",
    "# VOCAB_SIZE = 14999\n",
    "# for k, v in dictionary.items():\n",
    "#     if v < VOCAB_SIZE:\n",
    "#         word2idx[k] = v\n",
    "#     if v >= VOCAB_SIZE-1:\n",
    "#         continue\n",
    "\n",
    "# len(word2idx)\n",
    "# idx2word = {}\n",
    "# for k,v in word2idx.items():\n",
    "#     idx2word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(word2idx.items(),columns={\"word\",\"index\"}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(input_seq).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(target_seq).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Maximum length of sentences in input and target documents\n",
    "# max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
    "# max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n",
    "# print(\"max length of an encoder seq \" , max_encoder_seq_length)\n",
    "# print(\"max length of an decoder seq \", max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# encoder_input_data = pad_sequences(input_seq, maxlen=max_encoder_seq_length, dtype='int32', padding='post', truncating='post')\n",
    "# decoder_input_data = pad_sequences(target_seq, maxlen=max_decoder_seq_length, dtype='int32', padding='post', truncating='post')\n",
    "# print(\"max dcoder input: \",pd.DataFrame(decoder_input_data).shape)\n",
    "# print(\"max encoder input: \",pd.DataFrame(encoder_input_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(encoder_input_data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(decoder_input_data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.zeros((len(word2idx) + 1, 50))\n",
    "# print(\"embedding_matrix shape: \",embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word, i in word2idx.items():\n",
    "#     embedding_vector = embeddings_index.get(word.lower())\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "# pd.DataFrame(embedding_matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Embedding\n",
    "# embedding_layer = Embedding(len(word2idx) +1,\n",
    "#                             50,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=max_encoder_seq_length,\n",
    "#                             trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input_data = np.zeros(\n",
    "#     (len(input_seq), max_encoder_seq_length),\n",
    "#     dtype='float32')\n",
    "\n",
    "# decoder_input_data = np.zeros(\n",
    "#     (len(target_seq), max_decoder_seq_length),\n",
    "#     dtype='float32')\n",
    "\n",
    "# decoder_target_data = np.zeros(\n",
    "#     (len(target_seq), max_decoder_seq_length),\n",
    "#     dtype='float32')\n",
    "\n",
    "# for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
    "#     for timestep, token in enumerate(input_doc.split()):\n",
    "#         encoder_input_data[line,timestep] =  embedding_matrix[input_seq[line][timestep]]\n",
    "#     for timestep, token in enumerate(target_doc.split()):\n",
    "#         decoder_input_data[line,timestep] = embedding_matrix[target_seq[line][timestep]]\n",
    "#         if timestep > 0:\n",
    "#             decoder_target_data[line, timestep - 1] = embedding_matrix[target_seq[line][timestep]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_output_data = np.zeros(shape=\n",
    "#                                (len(target_seq), max_decoder_seq_length, len(word2idx)+1), \n",
    "#                                dtype=\"float32\")\n",
    "# for i, seqs in enumerate(decoder_input_data):\n",
    "#     for j, seq in enumerate(seqs):\n",
    "#         if j > 0:\n",
    "#              decoder_output_data[i][j-1][seq] = 1.\n",
    "# print(decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "# from tensorflow.keras.models import Model\n",
    "# #Dimensionality\n",
    "# dimensionality = 256\n",
    "# #The batch size and number of epochs\n",
    "# batch_size = 10\n",
    "# epochs = 150\n",
    "\n",
    "\n",
    "# #Encoder\n",
    "# encoder_inputs = Input(shape=(max_encoder_seq_length,))\n",
    "# embedded_encoder_inputs = embedding_layer(encoder_inputs)\n",
    "# encoder_lstm = LSTM(300, return_state=True)\n",
    "# encoder_outputs, state_hidden, state_cell = encoder_lstm(embedded_encoder_inputs)\n",
    "# encoder_states = [state_hidden, state_cell]\n",
    "\n",
    "\n",
    "# #Decoder\n",
    "# decoder_inputs = Input(shape=(max_decoder_seq_length,))\n",
    "# embeded_decoder_inputs = Embedding(len(word2idx)+1 ,\n",
    "#                             50,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=max_decoder_seq_length,\n",
    "#                             trainable=False)(decoder_inputs)\n",
    "                            \n",
    "# decoder_lstm = LSTM(300, return_sequences=True, return_state=True)\n",
    "# decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(embeded_decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# decoder_dense = Dense(len(word2idx)+1, activation='softmax')\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# embeded_decoder_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# #Model\n",
    "# training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# #plot_model(training_model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)\n",
    "# training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Compiling\n",
    "# training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "# #Training\n",
    "# training_model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size = 10, epochs = 5, validation_split = 0.2)\n",
    "# training_model.save('training_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "# training_model = load_model('training_model.h5')\n",
    "# encoder_inputs = training_model.input[0]\n",
    "# encoder_outputs, state_h_enc, state_c_enc = training_model.layers[4].output\n",
    "# encoder_states = [state_h_enc, state_c_enc]\n",
    "# encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = 300\n",
    "\n",
    "\n",
    "# decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "\n",
    "# decoder_outputs, state_hidden, state_cell = decoder_lstm(embeded_decoder_inputs, initial_state=decoder_states_inputs)\n",
    "# decoder_states = [state_hidden, state_cell]\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "# decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def decode_response(test_input):\n",
    "#     #Getting the output states to pass into the decoder\n",
    "#     states_value = encoder_model.predict(test_input)\n",
    "#     #Generating empty target sequence of length 1\n",
    "#     target_seq = np.zeros((1, max_decoder_seq_length))\n",
    "#     #Setting the first token of target sequence with the start token\n",
    "#     target_seq[0 , -1]= word2idx[\"end\"]\n",
    "#     #A variable to store our response word by word\n",
    "#     answer = []\n",
    "#     stop_condition = False\n",
    "#     lenOfSentence = 1;\n",
    "    \n",
    "#     while not stop_condition:\n",
    "   \n",
    "#         output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq , states_value])\n",
    "   \n",
    "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "#         print(output_tokens[0,-1,:])\n",
    "#         if(sampled_token_index == 0):\n",
    "#             sampled_token=\"\"\n",
    "#         else:\n",
    "#             sampled_token = idx2word[sampled_token_index]\n",
    "\n",
    "#         answer.append(sampled_token)\n",
    "\n",
    "#         if (sampled_token == '<END> ' or lenOfSentence > max_decoder_seq_length):\n",
    "#             stop_condition = True\n",
    "\n",
    "#         target_seq = np.zeros((1, max_decoder_seq_length))\n",
    "#         target_seq[0,-1] = word2idx[\"end\"]\n",
    "\n",
    "#         states_value = [hidden_state, cell_state]\n",
    "#         lenOfSentence +=1\n",
    "#     return decoded_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = np.zeros(shape = (1 , ))\n",
    "# temp = tokenizer.texts_to_sequences([\" hi how are you\"])\n",
    "# pd.DataFrame(temp)\n",
    "# temp = pad_sequences(temp, maxlen=20, dtype='int32', padding='post', truncating='post')\n",
    "# print(temp)\n",
    "# target_seq = np.zeros((1, 1 , 300))\n",
    "#     #Setting the first token of target sequence with the start token\n",
    "# target_seq[0, 0, word2idx['end']] = 1\n",
    "#     #A variable to store our response word by word\n",
    "# decoded_sentence = ''\n",
    "    \n",
    "# stop_condition = False\n",
    "# decode_response(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ChatBot:\n",
    "#   negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n",
    "#   exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "#     #Method to start the conversation\n",
    "#   def start_chat(self):\n",
    "#     user_response = input(\"Hi, I'm a chatbot trained on random dialogs. Would you like to chat with me?\\n\")\n",
    "    \n",
    "#     if user_response in self.negative_responses:\n",
    "#       print(\"Ok, have a great day!\")\n",
    "#       return\n",
    "#     self.chat(user_response)#Method to handle the conversation\n",
    "#   def chat(self, reply):\n",
    "#     while not self.make_exit(reply):\n",
    "#       reply = input(self.generate_response(reply)+\"\\n\")\n",
    "    \n",
    "#   #Method to convert user input into a matrix\n",
    "#   def string_to_matrix(self, user_input):\n",
    "#     tokens = clean_text(user_input).split()\n",
    "#     user_input_matrix = np.zeros(\n",
    "#       (1, max_encoder_seq_length),\n",
    "#       dtype='float32')\n",
    "#     user_input_embedded = pad_sequences(user_input, maxlen=max_encoder_seq_length, dtype='int32', padding='post', truncating='post')\n",
    "#     user_input_matrix = tokenizer.texts_to_sequences([user_input_embedded])\n",
    "#     return user_input_matrix\n",
    "  \n",
    "#   #Method that will create a response using seq2seq model we built\n",
    "#   def generate_response(self, user_input):\n",
    "#     input_matrix = self.string_to_matrix(user_input)\n",
    "#     chatbot_response = decode_response(input_matrix)\n",
    "#     #Remove <START> and <END> tokens from chatbot_response\n",
    "#     chatbot_response = chatbot_response.replace(\"<START>\",'')\n",
    "#     chatbot_response = chatbot_response.replace(\"<END>\",'')\n",
    "#     return chatbot_response#Method to check for exit commands\n",
    "#   def make_exit(self, reply):\n",
    "#     for exit_command in self.exit_commands:\n",
    "#       if exit_command in reply:\n",
    "#         print(\"Ok, have a great day!\")\n",
    "#         return True\n",
    "#     return False\n",
    "  \n",
    "# chatbot = ChatBot()\n",
    "# chatbot.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_index = {}\n",
    "# f = open(os.path.join(path, 'glove.6B.50d.txt'))\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
